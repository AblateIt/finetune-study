# Comparing QLora, Lora, and Full Fine-tuning
Comprehensive analysis of difference in performance of QLora, Lora, and Full Finetunes.


## Installation
### 1. Install python 3.9
### 2. Install pytorch stable:
>https://pytorch.org/get-started/locally/
### 3. Install axolotl
```
git clone https://github.com/OpenAccess-AI-Collective/axolotl
```
### 4. Install dependencies
```
pip3 install -e axolotl/.
pip3 install -U git+https://github.com/huggingface/peft.git
```
## For contributors running sweeps and training
### 1. Request access to the [AblateIt WandB Team](https://wandb.ai/ablateit)
### 2. Log into wandb through the CLI
    wandb login


## Links
- [Discord](https://discord.gg/HfNctSTJ)
- [HuggingFace](https://huggingface.co/AblateIt)
- [WandB](https://wandb.ai/ablateit)
