# Comparing QLora, Lora, and Full Fine-tuning
Comprehensive analysis of difference in performance of QLora, Lora, and Full Finetunes.


## Installation
### 1. Install python 3.9
### 2. Install pytorch stable:
>https://pytorch.org/get-started/locally/
<<<<<<< HEAD
### 3. Install axolotl and dependencies
git clone https://github.com/AblateIt/axolotl.git
pip3 install -e axolotl/.
pip3 install -U git+https://github.com/huggingface/peft.git
```
## For contributors running sweeps and training
### 1. Request access to the AblateIt WandB and HuggingFace teams 
### 2. Log into wandb and HuggingFace through the CLI
    wandb login
    huggingface-cli login



## Links
- [Discord](https://discord.gg/HfNctSTJ)
- [HuggingFace](https://huggingface.co/AblateIt)
- [WandB](https://wandb.ai/ablateit)
